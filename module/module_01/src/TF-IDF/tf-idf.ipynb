{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a264b2ab",
   "metadata": {},
   "source": [
    "# TF-IDF 从零讲解与实现\n",
    "本 Notebook 从零理解 TF-IDF（Term Frequency – Inverse Document Frequency）的原理，并通过 Python 实现从原理推导到 sklearn 实用的完整流程。\n",
    "\n",
    "主要内容：\n",
    "1. 什么是 TF-IDF？\n",
    "2. 从零实现 TF-IDF\n",
    "3. 使用 scikit-learn 的 `TfidfVectorizer`\n",
    "4. 可视化与应用示例（文本分类）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745d424",
   "metadata": {},
   "source": [
    "## 一、TF-IDF 的基本思想\n",
    "\n",
    "TF-IDF 是一种衡量词语在文档中重要性的权重方法\n",
    "- **TF（Term Frequency）**：词频，某个词在文档中出现的频率\n",
    "- **IDF（Inverse Document Frequency）**：逆文档频率，该词在整个语料中出现的稀有程度\n",
    "\n",
    "计算方式：\n",
    "\n",
    "$$\n",
    "\\text{TF}(t,d) = \\frac{\\text{count}(t, d)}{\\sum_t \\text{count}(t, d)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\frac{N}{1 + n_t} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $N$ ：总文档数\n",
    "- $n_t$ ：包含词 $t$ 的文档数\n",
    "- $\\text{TF-IDF}(t, d)$：词 $t$ 在文档 $d$ 中的重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6ef41",
   "metadata": {},
   "source": [
    "## 二、代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38d7fe",
   "metadata": {},
   "source": [
    "#### 1. 手动实现 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents: [['我', '喜欢', '机器', '学习'], ['机器', '学习', '很好玩'], ['我', '不', '喜欢', '学习']]\n",
      "TF List: [{'我': 0.25, '喜欢': 0.25, '机器': 0.25, '学习': 0.25}, {'机器': 0.3333333333333333, '学习': 0.3333333333333333, '很好玩': 0.3333333333333333}, {'我': 0.25, '不': 0.25, '喜欢': 0.25, '学习': 0.25}]\n",
      "All Unique Words: {'机器', '很好玩', '我', '学习', '喜欢', '不'}\n",
      "文档 1 TF-IDF:\n",
      "  我: 0.2500\n",
      "  喜欢: 0.2500\n",
      "  机器: 0.2500\n",
      "  学习: 0.1781\n",
      "\n",
      "文档 2 TF-IDF:\n",
      "  很好玩: 0.4685\n",
      "  机器: 0.3333\n",
      "  学习: 0.2374\n",
      "\n",
      "文档 3 TF-IDF:\n",
      "  不: 0.3514\n",
      "  我: 0.2500\n",
      "  喜欢: 0.2500\n",
      "  学习: 0.1781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter # For counting term frequencies\n",
    "\n",
    "docs = [\n",
    "    \"我 喜欢 机器 学习\",\n",
    "    \"机器 学习 很好玩\",\n",
    "    \"我 不 喜欢 学习\"\n",
    "]\n",
    "\n",
    "# Pre-Step: The documents after tokenization (tokens/terms separated by spaces)\n",
    "tokenized_docs = [doc.split() for doc in docs]\n",
    "print(\"Tokenized Documents:\", tokenized_docs)\n",
    "\n",
    "# Step 1: Count total number of documents\n",
    "N = len(tokenized_docs)\n",
    "\n",
    "# Step 2: Calculate TF(that is, token/term frequency) for each document\n",
    "tf_list = []\n",
    "for doc in tokenized_docs:\n",
    "    counts = Counter(doc) # return a dictionary of term frequencies, e.g., {'我': 1, '喜欢': 1, '机器': 1, '学习': 1}\n",
    "    total = len(doc)\n",
    "    tf = {word: counts[word] / total for word in counts}\n",
    "    tf_list.append(tf)\n",
    "print(\"TF List:\", tf_list)\n",
    "\n",
    "# Step 3: Calculate IDF (inverse document frequency) for each token/term\n",
    "# - If word appears in df documents, then idf = log(N / (1 + df)) + 1\n",
    "all_words = set(word for doc in tokenized_docs for word in doc)\n",
    "print(\"All Unique Words:\", all_words)\n",
    "idf = {}\n",
    "for word in all_words:\n",
    "    df = sum(1 for doc in tokenized_docs if word in doc) # if word appears in the document, count it\n",
    "    idf[word] = math.log(N / (1 + df)) + 1\n",
    "\n",
    "# Step 4: Calculate TF-IDF for each document\n",
    "# - TF-IDF = TF * IDF\n",
    "tfidf_list = []\n",
    "for tf in tf_list:\n",
    "    tfidf = {word: tf[word] * idf[word] for word in tf}\n",
    "    tfidf_list.append(tfidf)\n",
    "\n",
    "# Step 5: Output the results\n",
    "for i, tfidf in enumerate(tfidf_list):\n",
    "    print(f\"文档 {i+1} TF-IDF:\")\n",
    "    for word, value in sorted(tfidf.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {word}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb98b25",
   "metadata": {},
   "source": [
    "#### 2. 使用 `sklearn.feature_extraction.text.TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c5f61",
   "metadata": {},
   "source": [
    "**TfidfVectorizer 原理说明（源码文档节选）**\n",
    "\n",
    "> **Tf** means *term-frequency* while **tf-idf** means *term-frequency times inverse document-frequency*.  \n",
    "> This is a common term weighting scheme in information retrieval, that has also found good use in document classification.  \n",
    ">\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document  \n",
    "> is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically  \n",
    "> less informative than features that occur in a small fraction of the training corpus.  \n",
    ">\n",
    "> The formula that is used to compute the tf-idf for a term *t* of a document *d* in a document set is  \n",
    "> \n",
    "> \\[\n",
    "> tfidf(t, d) = tf(t, d) \\times idf(t)\n",
    "> \\]  \n",
    ">\n",
    "> and the *idf* is computed as  \n",
    ">\n",
    "> \\[\n",
    "> idf(t) = \\log \\frac{n}{df(t)} + 1 \\quad (\\text{if } smooth\\_idf=False)\n",
    "> \\]  \n",
    ">\n",
    "> where *n* is the total number of documents in the document set and *df(t)* is the document frequency of *t*;  \n",
    "> the document frequency is the number of documents in the document set that contain the term *t*.  \n",
    "> The effect of adding \"1\" to the idf in the equation above is that terms with zero idf,  \n",
    "> i.e., terms that occur in all documents in a training set, will not be entirely ignored.  \n",
    ">\n",
    "> (Note that the idf formula above differs from the standard textbook notation  \n",
    "> that defines the idf as  \n",
    "> \\[\n",
    "> idf(t) = \\log \\frac{n}{df(t) + 1}\n",
    "> \\])  \n",
    ">\n",
    "> If `smooth_idf=True` (the default), the constant \"1\" is added to the numerator and denominator of the idf  \n",
    "> as if an extra document was seen containing every term in the collection exactly once,  \n",
    "> which prevents zero divisions:  \n",
    ">\n",
    "> \\[\n",
    "> idf(t) = \\log \\frac{1 + n}{1 + df(t)} + 1\n",
    "> \\]  \n",
    ">\n",
    "> Furthermore, the formulas used to compute tf and idf depend on parameter settings  \n",
    "> that correspond to the SMART notation used in IR as follows:\n",
    ">\n",
    "> - Tf is \"n\" (natural) by default, \"l\" (logarithmic) when `sublinear_tf=True`.  \n",
    "> - Idf is \"t\" when `use_idf=True`, \"n\" (none) otherwise.  \n",
    "> - Normalization is \"c\" (cosine) when `norm='l2'`, \"n\" (none) when `norm=None`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf7499",
   "metadata": {},
   "source": [
    "2-1. analyze by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9fdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term/Token Frequencies: {'我': 7, '喜': 2, '欢': 9, '机': 8, '器': 3, '学': 5, '习': 1, '很': 6, '好': 4, '玩': 10, '不': 0}\n",
      "Feature Names: ['不' '习' '喜' '器' '好' '学' '很' '我' '机' '欢' '玩']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.31173037 0.40140961 0.40140961 0.         0.31173037\n",
      "  0.         0.40140961 0.40140961 0.40140961 0.        ]\n",
      " [0.         0.26806191 0.         0.34517852 0.45386827 0.26806191\n",
      "  0.45386827 0.         0.34517852 0.         0.45386827]\n",
      " [0.53972482 0.31877017 0.41047463 0.         0.         0.31877017\n",
      "  0.         0.41047463 0.         0.41047463 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"我喜欢机器学习\",\n",
    "    \"机器学习很好玩\",\n",
    "    \"我不喜欢学习\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',    # analyze by characters\n",
    "    token_pattern=None, # when analyzer is 'char', token_pattern should be None\n",
    "    ngram_range=(1,1)   # using unigrams (single characters), can try (1,2) for bigrams, try (1,3) for trigrams, etc.\n",
    "    # You can try different `ngram_range` to see the effect\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Term/Token Frequencies:\", vectorizer.vocabulary_)\n",
    "\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eba4ad",
   "metadata": {},
   "source": [
    "可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ed249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>不</th>\n",
       "      <th>习</th>\n",
       "      <th>喜</th>\n",
       "      <th>器</th>\n",
       "      <th>好</th>\n",
       "      <th>学</th>\n",
       "      <th>很</th>\n",
       "      <th>我</th>\n",
       "      <th>机</th>\n",
       "      <th>欢</th>\n",
       "      <th>玩</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311730</td>\n",
       "      <td>0.401410</td>\n",
       "      <td>0.401410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.401410</td>\n",
       "      <td>0.401410</td>\n",
       "      <td>0.401410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345179</td>\n",
       "      <td>0.453868</td>\n",
       "      <td>0.268062</td>\n",
       "      <td>0.453868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539725</td>\n",
       "      <td>0.318770</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410475</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          不         习         喜         器         好         学         很  \\\n",
       "0  0.000000  0.311730  0.401410  0.401410  0.000000  0.311730  0.000000   \n",
       "1  0.000000  0.268062  0.000000  0.345179  0.453868  0.268062  0.453868   \n",
       "2  0.539725  0.318770  0.410475  0.000000  0.000000  0.318770  0.000000   \n",
       "\n",
       "          我         机         欢         玩  \n",
       "0  0.401410  0.401410  0.401410  0.000000  \n",
       "1  0.000000  0.345179  0.000000  0.453868  \n",
       "2  0.410475  0.000000  0.410475  0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 每一行代表一个文档，每一列代表一个特征词（词汇表中的词），数值为 TF-IDF 权重\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7e940",
   "metadata": {},
   "source": [
    "2-2. analyze by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['不' '喜欢' '学习' '很好玩' '我' '机器']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.52682017 0.40912286 0.         0.52682017 0.52682017]\n",
      " [0.         0.         0.42544054 0.72033345 0.         0.54783215]\n",
      " [0.63174505 0.4804584  0.37311881 0.         0.4804584  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"我 喜欢 机器 学习\",\n",
    "    \"机器 学习 很好玩\",\n",
    "    \"我 不 喜欢 学习\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', # analyze by words\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\", # tokenize by words, r\"(?u)\\b\\w+\\b\" means to match words\n",
    "    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8507262",
   "metadata": {},
   "source": [
    "2-3. 字符级与分词器示例（中文文本）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b1f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.670 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-level 特征： ['不' '不喜' '习' '习很' '喜' '喜欢' '器' '器学' '好' '好玩']\n",
      "Jieba 特征： ['不' '喜欢' '好玩' '学习' '很' '我' '机器']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def jieba_tokenize(text):\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "vectorizer_char = TfidfVectorizer(analyzer='char', ngram_range=(1,2))\n",
    "vectorizer_jieba = TfidfVectorizer(tokenizer=jieba_tokenize, token_pattern=None)\n",
    "\n",
    "X_char = vectorizer_char.fit_transform(docs)\n",
    "X_jieba = vectorizer_jieba.fit_transform(docs)\n",
    "\n",
    "print(\"Char-level 特征：\", vectorizer_char.get_feature_names_out()[:10])\n",
    "print(\"Jieba 特征：\", vectorizer_jieba.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a83c5",
   "metadata": {},
   "source": [
    "#### 3. 应用示例：TF-IDF + 朴素贝叶斯分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76953e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 样例数据\n",
    "corpus = [\n",
    "    \"我 爱 吃 苹果\",\n",
    "    \"我 爱 吃 香蕉\",\n",
    "    \"今天 天气 真好\",\n",
    "    \"明天 要 下雨 了\",\n",
    "    \"苹果 很 好吃\",\n",
    "    \"香蕉 味道 不错\"\n",
    "]\n",
    "y = [0, 0, 1, 1, 0, 0]  # 0: 水果类, 1: 天气类\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d234cd",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "1. TF-IDF 是一种基于统计的文本表示方法，用于衡量词语在文档中的重要性。\n",
    "2. 它综合考虑了词频（TF）与逆文档频率（IDF），能抑制常见词、突出关键词。\n",
    "3. 在实际应用中，可使用 `TfidfVectorizer` 自动完成分词、词频统计与矩阵生成。\n",
    "4. TF-IDF 向量常作为传统机器学习模型（如 SVM、NB、LR）的输入特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5ae3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
